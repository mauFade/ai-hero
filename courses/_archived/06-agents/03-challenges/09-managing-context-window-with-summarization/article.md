LLMs have a limited context window.

To handle extremely long context windows, you can summarize the conversation so far and feed the summary into the LLM.

This naturally means that as conversations get longer and longer, the LLMs output degrades.
